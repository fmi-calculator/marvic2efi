{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARVIC models with EFI standards\n",
    "\n",
    "EFI standards are concerned with two main files:\n",
    "\n",
    "1. The model output file which contains the actual predicted values from the model. This file will be in **csv** format. There is also a possibility to have the output file in netCDF format, but tabular data has certain advantages.\n",
    "\n",
    "2. The dataset metadata file contains all the accompanying metadata information that will help interpret and use the model outputs (resolution, dimensions, variable names, units etc. but also uncertainty propagation, data assimilation, model complexity etc.). This file will be in **json** format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with installing libraries\n",
    "\n",
    "Install and import Python equivalents of the R packages, including pandas, numpy, xarray for NetCDF handling (e.g. if your raw model outputs are in NetCDF), and pyarrow for Parquet files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import datetime\n",
    "import tempfile\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# External dependencies; all available on PyPI\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dicttoxml\n",
    "import xarray as xr\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "# optional\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing the model output file\n",
    "\n",
    "## Read your model outputs\n",
    "\n",
    "Read in your model outputs in your raw format. The example below assumes your outputs are in netcdf format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Model Outputs from NetCDF\n",
    "\n",
    "# Define the path to the NetCDF file\n",
    "raw_output_file = \".../my_raw_model_output.nc\"\n",
    "\n",
    "# Open the NetCDF file using xarray\n",
    "ds = xr.open_dataset(raw_output_file)\n",
    "\n",
    "# Extract variables from the dataset\n",
    "out_nee = ds[\"NEE\"].values  # Net Ecosystem Exchange\n",
    "out_gpp = ds[\"GPP\"].values  # Gross Primary Productivity\n",
    "out_ar = ds[\"AutoResp\"].values  # Autotrophic Respiration\n",
    "\n",
    "# Calculate Net Primary Productivity (NPP)\n",
    "out_npp = out_gpp - out_ar\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat Model Outputs to EFI Standards\n",
    "\n",
    "Convert outputs to a flat file format (CSV):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    datetime  parameter  obs_flag                variable    prediction  \\\n",
      "0 2018-01-01          1         1  net_ecosystem_exchange  8.429057e-09   \n",
      "1 2018-01-01          2         1  net_ecosystem_exchange  8.850567e-09   \n",
      "2 2018-01-01          3         1  net_ecosystem_exchange  9.294480e-09   \n",
      "3 2018-01-01          4         1  net_ecosystem_exchange  9.276512e-09   \n",
      "4 2018-01-01          5         1  net_ecosystem_exchange  9.312278e-09   \n",
      "\n",
      "   data_assimilation  forecast  \n",
      "0                  0         0  \n",
      "1                  0         0  \n",
      "2                  0         0  \n",
      "3                  0         0  \n",
      "4                  0         0  \n"
     ]
    }
   ],
   "source": [
    "# Reformat Model Outputs to EFI Standards\n",
    "\n",
    "# reference_datetime and pubtime can be the same for MARVIC\n",
    "# just putting today's date\n",
    "pub_datetime = reference_datetime = pd.Timestamp.today()\n",
    "\n",
    "# Define the actual time stamps for the predictions\n",
    "n_time = out_gpp.shape[1]\n",
    "start_date = pd.Timestamp(\"2018-01-01\")  # Replace with your simulation start date\n",
    "datetimes = pd.date_range(start=start_date, periods=n_time, freq=\"D\")\n",
    "\n",
    "# Define ensemble parameters\n",
    "n_ensembles = out_gpp.shape[0]\n",
    "ensembles = np.arange(1, n_ensembles + 1)\n",
    "\n",
    "obs_dim = 1  ## 1 = latent state\n",
    "## 2 = latent state + observation error\n",
    "\n",
    "variable_names = [\n",
    "    \"net_ecosystem_exchange\",\n",
    "    \"gross_primary_productivity\",\n",
    "    \"net_primary_productivity\",\n",
    "]\n",
    "\n",
    "# Prepare storage for reformatted data\n",
    "output_storage = np.empty((n_ensembles, n_time, len(variable_names)))\n",
    "\n",
    "output_storage[..., 0] = np.squeeze(out_nee)\n",
    "output_storage[..., 1] = np.squeeze(out_gpp)\n",
    "output_storage[..., 2] = np.squeeze(out_npp)\n",
    "\n",
    "# Define flags for data assimilation and forecast\n",
    "data_assimilation = np.zeros(n_time, dtype=int)  # 0 indicates 'free-run'\n",
    "forecast = np.zeros(n_time, dtype=int)  # 0 = hindcast, 1 = forecast\n",
    "\n",
    "# Wrangle data from array into a long format DataFrame\n",
    "df_combined = (\n",
    "    pd.DataFrame(\n",
    "        output_storage.reshape(-1, len(variable_names)), columns=variable_names\n",
    "    )\n",
    "    .assign(\n",
    "        datetime=np.repeat(datetimes, n_ensembles),\n",
    "        parameter=np.tile(ensembles, n_time),\n",
    "        obs_flag=obs_dim,\n",
    "    )\n",
    "    .melt(\n",
    "        id_vars=[\"datetime\", \"parameter\", \"obs_flag\"],\n",
    "        var_name=\"variable\",\n",
    "        value_name=\"prediction\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add data assimilation and forecast flags\n",
    "df_combined = df_combined.merge(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": datetimes,\n",
    "            \"data_assimilation\": data_assimilation,\n",
    "            \"forecast\": forecast,\n",
    "        }\n",
    "    ),\n",
    "    on=\"datetime\",\n",
    ")\n",
    "\n",
    "# Display the first few rows of the reformatted DataFrame\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          datetime  parameter  obs_flag                  variable  \\\n",
      "1682683 2023-12-31        252         1  net_primary_productivity   \n",
      "1682684 2023-12-31        253         1  net_primary_productivity   \n",
      "1682685 2023-12-31        254         1  net_primary_productivity   \n",
      "1682686 2023-12-31        255         1  net_primary_productivity   \n",
      "1682687 2023-12-31        256         1  net_primary_productivity   \n",
      "\n",
      "           prediction  data_assimilation  forecast  \n",
      "1682683 -1.631362e-09                  0         0  \n",
      "1682684 -1.623129e-09                  0         0  \n",
      "1682685 -1.659175e-09                  0         0  \n",
      "1682686 -1.684502e-09                  0         0  \n",
      "1682687 -1.538097e-09                  0         0  \n"
     ]
    }
   ],
   "source": [
    "print(df_combined.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write EFI csv\n",
    "\n",
    "Export the formatted DataFrame to CSV using pandas and to Parquet format using pyarrow/pandas with appropriate compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the CSV file\n",
    "# YOUR PATH HERE\n",
    "# we can discuss file naming\n",
    "csv_filename = \".../FI-Qvd_SPY-C_2018-2023_EFIstandard.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_combined.to_csv(csv_filename, index=False)\n",
    "\n",
    "# OPTIONAL\n",
    "### convert to parquet format\n",
    "# this is preliminary at the moment\n",
    "# included here more as a discussion point\n",
    "# FMI can convert your csv into parquet\n",
    "# we are currently discussing the S3 folder hierarchy\n",
    "# use cases to be discussed\n",
    "\n",
    "# Define the path for the Parquet file\n",
    "# YOUR PATH HERE\n",
    "# parquet_path = \".../parquet_output\"\n",
    "\n",
    "# Write the DataFrame as a partitioned dataset\n",
    "# ds.write_dataset(pa.Table.from_pandas(df_combined), parquet_path, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing the standard Metadata file\n",
    "\n",
    "EFI metadata convention builds on the Ecological Metadata Language (EML) metadata standard. EML has a long development history, is interconvertible with many other standards, and has built-in extensibility.\n",
    "\n",
    "EFI dataset metadata convention makes some core components of the base EML standard required or recommended. Many optional elements also exist as part of the EML schema (https://eml.ecoinformatics.org/schema/). The appendices of the EFI standards publication (https://doi.org/10.1002/ecs2.4686) includes the descriptions of these tags in more detail. We made these pdfs available on MARVIC share point also.\n",
    "\n",
    "## Prepare dataset metadata\n",
    "\n",
    "In R, there is the EML package but in python there is no package/function, so we have to generate the structures by hand:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: List of dicts\n",
    "attrList = [\n",
    "    {\n",
    "        \"attributeName\": \"datetime\",\n",
    "        \"attributeDefinition\": \"[dimension]{datetime}\",\n",
    "        \"storageType\": \"date\",\n",
    "        \"measurementScale\": {\n",
    "            \"dateTime\": {\"formatString\": \"YYYY-MM-DD\", \"dateTimeDomain\": []}\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"parameter\",\n",
    "        \"attributeDefinition\": \"[dimension]{index of ensemble member}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"dimensionless\"},\n",
    "                \"numericDomain\": {\"numberType\": \"integer\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"obs_flag\",\n",
    "        \"attributeDefinition\": \"[dimension]{observation error}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"dimensionless\"},\n",
    "                \"numericDomain\": {\"numberType\": \"integer\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"net_ecosystem_exchange\",\n",
    "        \"attributeDefinition\": \"[variable]{Net Ecosystem Exchange}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"kg C m-2 s-1\"},\n",
    "                \"numericDomain\": {\"numberType\": \"real\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"gross_primary_productivity\",\n",
    "        \"attributeDefinition\": \"[variable]{Gross Primary Productivity}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"kg C m-2 s-1\"},\n",
    "                \"numericDomain\": {\"numberType\": \"real\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"data_assimilation\",\n",
    "        \"attributeDefinition\": \"[flag]{whether time step assimilated data}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"dimensionless\"},\n",
    "                \"numericDomain\": {\"numberType\": \"integer\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"attributeName\": \"forecast\",\n",
    "        \"attributeDefinition\": \"[flag]{whether time step forecast or hindcast}\",\n",
    "        \"storageType\": \"float\",\n",
    "        \"measurementScale\": {\n",
    "            \"ratio\": {\n",
    "                \"unit\": {\"standardUnit\": \"dimensionless\"},\n",
    "                \"numericDomain\": {\"numberType\": \"integer\"},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical = {\n",
    "    \"objectName\": \".../FI-Qvd_SPY-C_2018-2023_EFIstandard.csv\",\n",
    "    \"size\": {\"unit\": \"bytes\", \"size\": \"110780\"},\n",
    "    \"authentication\": {\n",
    "        \"method\": \"MD5\",\n",
    "        \"authentication\": \"4dbe687fa1f5fc0ff789096076eebd78\",\n",
    "    },\n",
    "    \"dataFormat\": {\n",
    "        \"textFormat\": {\n",
    "            \"recordDelimiter\": \"\\n\",\n",
    "            \"attributeOrientation\": \"column\",\n",
    "            \"simpleDelimited\": {\"fieldDelimiter\": \",\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "dataTable = {\n",
    "    \"entityName\": \"MARVIC T3.2? outputs\",  ## this is a standard name to allow us to distinguish this entity from\n",
    "    \"entityDescription\": \"Agro-ecosystem carbon budget predictions\",\n",
    "    \"physical\": physical,\n",
    "    \"attributeList\": attrList,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who to contact about this output\n",
    "creator_info = {\n",
    "    \"individualName\": {\"givenName\": \"YourName\", \"surName\": \"YourSurname\"},\n",
    "    \"electronicMailAddress\": \"YourEmail\",\n",
    "    \"id\": \"https://orcid.org/YourOrcid\",\n",
    "}\n",
    "\n",
    "# # can also set taxonomic, temporal, and geographic coverage of the outputs\n",
    "# UPDATE ACCORDINGLY\n",
    "coverage = {\n",
    "    \"geographicCoverage\": {\n",
    "        \"geographicDescription\": \"Qvidja, Finland\",\n",
    "        \"boundingCoordinates\": {\n",
    "            \"westBoundingCoordinate\": 22.39017,\n",
    "            \"eastBoundingCoordinate\": 22.3932251,\n",
    "            \"northBoundingCoordinate\": 60.29531,\n",
    "            \"southBoundingCoordinate\": 60.29331,\n",
    "        },\n",
    "    },\n",
    "    \"temporalCoverage\": {\n",
    "        \"rangeOfDates\": {\n",
    "            \"beginDate\": {\"calendarDate\": str(datetimes[0])},\n",
    "            \"endDate\": {\"calendarDate\": str(datetimes[-1])},\n",
    "        }\n",
    "    },\n",
    "    \"taxonomicCoverage\": {\n",
    "        \"taxonomicClassification\": {\n",
    "            \"taxonRankName\": \"Genus\",\n",
    "            \"taxonRankValue\": \"Triticum\",\n",
    "            \"taxonomicClassification\": {\n",
    "                \"taxonRankName\": \"Species\",\n",
    "                \"taxonRankValue\": \"aestivum\",\n",
    "            },\n",
    "        },\n",
    "        \"taxonomicClassification\": {\n",
    "            \"taxonRankName\": \"Genus\",\n",
    "            \"taxonRankValue\": \"Phleum\",\n",
    "            \"taxonomicClassification\": {\n",
    "                \"taxonRankName\": \"Species\",\n",
    "                \"taxonRankValue\": \"pratense\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# # Set key words.  We will need to develop a MARVIC controlled vocabulary\n",
    "keywordSet = [\n",
    "    {\n",
    "        \"keywordThesaurus\": \"MARVIC controlled vocabulary\",\n",
    "        \"keyword\": [\"hindcast\", \"fluxes\", \"timeseries\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above bits to document the output dataset as a whole:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"title\": \"MARVIC T3.2 model outputs\",\n",
    "    \"creator\": creator_info,\n",
    "    \"pubDate\": reference_datetime,\n",
    "    \"intellectualRights\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    \"abstract\": \"An illustration of how we might use EML metadata to describe a MARVIC output\",\n",
    "    \"dataTable\": dataTable,\n",
    "    \"keywordSet\": keywordSet,\n",
    "    \"coverage\": coverage,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare additional metadata\n",
    "\n",
    "EFI standards has additional metadata tags to store forecast specific information for basic elements and model structure & uncertainty:\n",
    "\n",
    "- `target_id`: this is a unique identifier (e.g., URL or DOI) that links to data or metadata about what the forecast/prediction is being scored against\n",
    "\n",
    "- `model_version`: This identifier should update when the model is updated or when the underlying modeling **workflow** is updated. EFI recommends issuing DOIs for different model/workflow versions, and thus, this is a natural choice for a model_version.\n",
    "\n",
    "- `iteration_id`: represents a unique ID for each run. Examples might be a start time or database ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional metadata for EFI standards\n",
    "## Global attributes\n",
    "target_id = \"YOUR_TARGET_ID\"\n",
    "model_name = \"YOUR_MODEL_NAME\"\n",
    "model_version = \"YOUR_MODEL_VERSION\"\n",
    "iteration_id = \"20180101T000000\"  # UPDATE ACCORDINGLY\n",
    "\n",
    "my_horizon = f\"{n_time} days\"  # UPDATE ACCORDINGLY\n",
    "\n",
    "additionalMetadata = {\n",
    "    \"metadata\": {\n",
    "        \"forecast\": {\n",
    "            ## Basic elements\n",
    "            \"timestep\": \"1 day\",  ## should be udunits parsable; change if not days\n",
    "            \"horizon\": my_horizon,\n",
    "            \"reference_datetime\": reference_datetime,\n",
    "            \"iteration_id\": iteration_id,\n",
    "            \"target_id\": target_id,\n",
    "            \"metadata_standard_version\": \"1.0\",\n",
    "            \"model_description\": {\n",
    "                \"model_id\": model_version,\n",
    "                \"name\": model_name,\n",
    "                \"type\": \"process-based\",\n",
    "                \"repository\": \"https://github.com/YOUR_REPO\",\n",
    "            },\n",
    "            ## MODEL STRUCTURE & UNCERTAINTY CLASSES\n",
    "            \"initial_conditions\": {\"present\": False},\n",
    "            \"drivers\": {\"present\": False},\n",
    "            \"parameters\": {\n",
    "                \"present\": True,\n",
    "                \"date_driven\": True,\n",
    "                \"complexity\": 4,  ## number of parameters being varied, UPDATE ACCORDINGLY\n",
    "                \"propagation\": {\"type\": \"ensemble\", \"size\": 256},  # UPDATE ACCORDINGLY\n",
    "            },\n",
    "            \"random_effects\": {\"present\": False},\n",
    "            \"process_error\": {\"present\": False},\n",
    "            \"obs_error\": {\"present\": False},\n",
    "        }  # forecast\n",
    "    }  # metadata\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write metadata file: In python, there are no nice utilities, so we have to create the XML by hand.\n",
    "This is significantly simplified by the `dicttoxml` package, though we still have to do a bit of extra work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_xml = ET.fromstring(\n",
    "    dicttoxml.dicttoxml(dataset, custom_root=\"dataset\", attr_type=False)\n",
    ")\n",
    "\n",
    "metadata_xml = ET.fromstring(\n",
    "    dicttoxml.dicttoxml(\n",
    "        additionalMetadata, custom_root=\"additionalMetadata\", attr_type=False\n",
    "    )\n",
    ")\n",
    "\n",
    "my_eml = ET.Element(\n",
    "    \"eml:eml\",\n",
    "    {\n",
    "        \"xmlns:eml\": \"https://eml.ecoinformatics.org/eml-2.2.0\",\n",
    "        \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "        \"xmlns:stmml\": \"http://www.xml-cml.org/schema/stmml-1.2\",\n",
    "        \"system\": \"datetime\",\n",
    "        \"packageId\": iteration_id,\n",
    "        \"xsi:schemaLocation\": \"https://eml.ecoinformatics.org/eml-2.2.0 https://eml.ecoinformatics.org/eml-2.2.0/eml.xsd\",\n",
    "    },\n",
    ")\n",
    "\n",
    "my_eml.append(dataset_xml)\n",
    "my_eml.append(metadata_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write metadata to XML and JSON files\n",
    "eml_filename = \".../my_model.xml\"\n",
    "with open(eml_filename, \"w\") as f:\n",
    "    f.write(ET.tostring(my_eml, encoding=\"unicode\"))\n",
    "\n",
    "# Convert XML to JSON\n",
    "xml_string = ET.tostring(my_eml, encoding=\"unicode\")\n",
    "xml_dict = xmltodict.parse(xml_string)\n",
    "\n",
    "# Write to JSON file\n",
    "json_filename = \".../my_model.json\"\n",
    "with open(json_filename, \"w\") as f:\n",
    "    json.dump(xml_dict, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
